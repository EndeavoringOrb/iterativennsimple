{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peformance comparison of different algorithms\n",
    "\n",
    "In this notebook we analyze the performance of various algorithms, including MLPs, dense dynamical systems, and various sorts of sparse dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# TODO Add these imports and test MaskeLinear and SparseLinear\n",
    "from iterativennsimple.MaskedLinear import MaskedLinear\n",
    "from iterativennsimple.SparseLinear import SparseLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(size, entries, device=\"cuda\"):\n",
    "    \"\"\"create a variety of sparse matrices with the same entries\n",
    "\n",
    "    Args:\n",
    "        size (int, optional): Size of the square matrix. Defaults to 1000.\n",
    "        entries (int, optional): Total number of non-zero entries. Note this is an upper bound, but should be close to the actual size. \n",
    "        device (str, optional): \"cuda\" or \"cpu\". Defaults to \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of the sparse matrices\n",
    "    \"\"\" \n",
    "    output = {}\n",
    "    # We first create a COO tensor since that is easier to create\n",
    "\n",
    "    # We create a set of random indices and values\n",
    "    # Each index is a pair of (row, column) indices that indicate the location of the non-zero entry \n",
    "    # whose value is stored in the vals tensor.\n",
    "    # NOTE: This is memory efficient in that we never need to create a dense matrix\n",
    "    indices = torch.randint(0, size, (entries,2))\n",
    "    vals = torch.randn(entries)\n",
    "\n",
    "    # We then create the COO tensor\n",
    "    coo = torch.sparse_coo_tensor(indices.t(), vals, (size, size), device=device)\n",
    "\n",
    "    # We then coalesce it to make sure there are no duplicate indices\n",
    "    coo = coo.coalesce()\n",
    "\n",
    "    # Then we convert it to CSC and CSR, which are two common sparse matrix formats\n",
    "    # NOTE: CSR is what we really want for actual use, since it is the most efficient for matrix-vector multiplication on a GPU\n",
    "    csc = coo.to_sparse_csc()\n",
    "    csr = coo.to_sparse_csr()\n",
    "\n",
    "    # We also create a dense version of the matrix for comparison\n",
    "    # NOTE: This is not memory efficient, since many entries will be 0 and not really needed, but is useful for comparison\n",
    "    dense = coo.to_dense()\n",
    "\n",
    "    # We also create the MaskedLinear and SparseLinear objects\n",
    "\n",
    "    # NOTE: Matrices and modules are transposed in PyTorch, see for example https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    # This is why we transpose the input to and output from the module when we define the matmul operator \"@\" below\n",
    "    class moduleWrapper(object):\n",
    "        def __init__(self, module, device=device):\n",
    "            self.module = module\n",
    "            self.device = device\n",
    "        def __matmul__(self, x):\n",
    "            return self.module(x.T).T\n",
    "        \n",
    "    maskedLinear = moduleWrapper(MaskedLinear.from_coo(coo).to(device), device)\n",
    "    sparseLinear = moduleWrapper(SparseLinear.from_coo(coo).to(device), device)\n",
    "\n",
    "    output[\"dense\"] = dense\n",
    "    output[\"coo\"] = coo\n",
    "    output[\"csc\"] = csc\n",
    "    output[\"csr\"] = csr\n",
    "    output[\"maskedLinear\"] = maskedLinear\n",
    "    output[\"sparseLinear\"] = sparseLinear\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check that all of the matrices are the same operator\n",
    "def matrix_check(size, entries, device):\n",
    "    # Generate the matrices\n",
    "    matrices = generate(size, entries, device=device)\n",
    "    # Size of the RHS\n",
    "    x_cols = 100\n",
    "    x = torch.randn(size, x_cols, device=device)\n",
    "\n",
    "    print('Matrix-matrix multiplication check')\n",
    "    y_true = None\n",
    "    base_name = None\n",
    "\n",
    "    for matrix, matrix_name in zip(matrices.values(), matrices.keys()):\n",
    "        y = matrix @ x\n",
    "        if y_true is None:\n",
    "            y_true = y\n",
    "            base_name = matrix_name\n",
    "        else:\n",
    "            # print the frobenius norm of the difference\n",
    "            print(f\"||{matrix_name} - {base_name}||_F: {torch.norm(y-y_true)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-matrix multiplication check\n",
      "||coo - dense||_F: 0.00014523211575578898\n",
      "||csc - dense||_F: 0.00014523512800224125\n",
      "||csr - dense||_F: 0.00014524077414534986\n",
      "||maskedLinear - dense||_F: 0.00013298098929226398\n",
      "||sparseLinear - dense||_F: 0.00016632354527246207\n"
     ]
    }
   ],
   "source": [
    "matrix_check(1000, 23*1000, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timing(size, entries, device, syncgpu):\n",
    "    # test if cuda is available\n",
    "    if device == \"cuda\":\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"CUDA is not available, using CPU instead\")\n",
    "            device = \"cpu\"\n",
    "\n",
    "    print(f\"Running on {device}\")\n",
    "    print(f\"Size: {size}\")\n",
    "    print(f\"Entries: {entries}\")\n",
    "    print(f\"Synchronize GPU: {syncgpu}\")\n",
    "    \n",
    "    # Generate the matrices\n",
    "    matrices = generate(size, entries, device=device)\n",
    "\n",
    "    # Size of the RHS\n",
    "    x_cols = 100\n",
    "\n",
    "    # Compute the timings\n",
    "    print('Matrix-matrix multiplication timings:')\n",
    "    base_time = None\n",
    "    base_name = None\n",
    "    for matrix, matrix_name in zip(matrices.values(), matrices.keys()):\n",
    "        # first, do a few runs to warm up the cache\n",
    "        for i in range(2):\n",
    "            x = torch.randn(size, x_cols, device=device)\n",
    "            y = matrix @ x\n",
    "        # now do the timings\n",
    "        matrix_times = []\n",
    "        for i in range(5):\n",
    "            x = torch.randn(size, x_cols, device=device)    \n",
    "            if syncgpu:\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            y = matrix @ x\n",
    "            if syncgpu:\n",
    "                torch.cuda.synchronize()\n",
    "            matrix_time = time.perf_counter()-start\n",
    "            matrix_times.append(matrix_time)\n",
    "        avg_matrix_time = sum(matrix_times)/len(matrix_times)\n",
    "        min_matrix_time = min(matrix_times)\n",
    "        max_matrix_time = max(matrix_times)\n",
    "        if base_time is None:\n",
    "            base_time = avg_matrix_time\n",
    "            base_name = matrix_name\n",
    "\n",
    "        print(f\"{matrix_name} avg time:\", avg_matrix_time)\n",
    "        print(f\"{matrix_name} min time:\", min_matrix_time)\n",
    "        print(f\"{matrix_name} max time:\", max_matrix_time)\n",
    "        print(f\"{matrix_name} speedup over {base_name}:\", base_time/avg_matrix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Size: 10000\n",
      "Entries: 100000\n",
      "Synchronize GPU: True\n",
      "Matrix-matrix multiplication timings:\n",
      "dense avg time: 0.0005857189185917378\n",
      "dense min time: 0.0005834228359162807\n",
      "dense max time: 0.0005869702436029911\n",
      "dense speedup over dense: 1.0\n",
      "coo avg time: 0.000102210883051157\n",
      "coo min time: 0.00010054977610707283\n",
      "coo max time: 0.00010390579700469971\n",
      "coo speedup over dense: 5.730494650932453\n",
      "csc avg time: 0.0002488784492015839\n",
      "csc min time: 0.0002381298691034317\n",
      "csc max time: 0.0002848380245268345\n",
      "csc speedup over dense: 2.353433655950353\n",
      "csr avg time: 3.7641171365976335e-05\n",
      "csr min time: 3.7390273064374924e-05\n",
      "csr max time: 3.796210512518883e-05\n",
      "csr speedup over dense: 15.56059222750879\n",
      "maskedLinear avg time: 0.0032173518091440203\n",
      "maskedLinear min time: 0.003213451709598303\n",
      "maskedLinear max time: 0.003219163976609707\n",
      "maskedLinear speedup over dense: 0.18205000675619895\n",
      "sparseLinear avg time: 0.00016622431576251984\n",
      "sparseLinear min time: 0.00016529299318790436\n",
      "sparseLinear max time: 0.00016708578914403915\n",
      "sparseLinear speedup over dense: 3.523665691778443\n"
     ]
    }
   ],
   "source": [
    "size = 10000\n",
    "sparsity = 0.001\n",
    "run_timing(10000, int(10000*10000*sparsity), \"cuda\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
