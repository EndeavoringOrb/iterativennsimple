{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peformance comparison of different algorithms\n",
    "\n",
    "In this notebook we analyze the performance of various algorithms, including MLPs, dense dynamical systems, and various sorts of sparse dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# TODO Add these imports and test MaskeLinear and SparseLinear\n",
    "from iterativennsimple.MaskedLinear import MaskedLinear\n",
    "from iterativennsimple.SparseLinear import SparseLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(size, entries, device=\"cuda\"):\n",
    "    \"\"\"create a variety of sparse matrices \n",
    "\n",
    "    Args:\n",
    "        size (int, optional): Size of the square matrix. Defaults to 1000.\n",
    "        entries (int, optional): Total number of non-zero entries. Note this is an upper bound, but should be close to the actual size. Defaults to 23*1000.\n",
    "        device (str, optional): \"cuda\" or \"cpu\". Defaults to \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of the sparse matrices\n",
    "    \"\"\" \n",
    "    output = {}\n",
    "    # We first create a COO tensor since that is easier to create\n",
    "    indices = torch.randint(0, size, (entries,2))\n",
    "    vals = torch.randn(entries)\n",
    "    coo = torch.sparse_coo_tensor(indices.t(), vals, (size, size), device=device)\n",
    "    coo = coo.coalesce()\n",
    "\n",
    "    # Then we convert it to CSC, CSR and dense\n",
    "    dense = coo.to_dense()\n",
    "    csc = coo.to_sparse_csc()\n",
    "    csr = coo.to_sparse_csr()\n",
    "    \n",
    "    # We also create the MaskedLinear and SparseLinear objects\n",
    "    class moduleWrapper(object):\n",
    "        def __init__(self, module, device=device):\n",
    "            self.module = module\n",
    "            self.device = device\n",
    "        def __matmul__(self, x):\n",
    "            return self.module(x.T).T\n",
    "        \n",
    "    maskedLinear = moduleWrapper(MaskedLinear.from_coo(coo).to(device), device)\n",
    "    sparseLinear = moduleWrapper(SparseLinear.from_coo(coo).to(device), device)\n",
    "\n",
    "    output[\"dense\"] = dense\n",
    "    output[\"coo\"] = coo\n",
    "    output[\"csc\"] = csc\n",
    "    output[\"csr\"] = csr\n",
    "    output[\"maskedLinear\"] = maskedLinear\n",
    "    output[\"sparseLinear\"] = sparseLinear\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check that all of the matrices are the same operator\n",
    "def matrix_check(size, entries, device):\n",
    "    # Generate the matrices\n",
    "    matrices = generate(size, entries, device=device)\n",
    "    # Size of the RHS\n",
    "    x_cols = 100\n",
    "    x = torch.randn(size, x_cols, device=device)\n",
    "\n",
    "    print('Matrix-matrix multiplication check')\n",
    "    y_true = None\n",
    "    base_name = None\n",
    "\n",
    "    for matrix, matrix_name in zip(matrices.values(), matrices.keys()):\n",
    "        y = matrix @ x\n",
    "        if y_true is None:\n",
    "            y_true = y\n",
    "            base_name = matrix_name\n",
    "        else:\n",
    "            # print the frobenius norm of the difference\n",
    "            print(f\"||{matrix_name} - {base_name}||_F: {torch.norm(y-y_true)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-matrix multiplication check\n",
      "||coo - dense||_F: 0.00014353354345075786\n",
      "||csc - dense||_F: 0.00014352313883136958\n",
      "||csr - dense||_F: 0.00014348502736538649\n",
      "||maskedLinear - dense||_F: 0.00013009528629481792\n",
      "||sparseLinear - dense||_F: 0.0001639742695260793\n"
     ]
    }
   ],
   "source": [
    "matrix_check(1000, 23*1000, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timing(size, entries, device, syncgpu):\n",
    "    # test if cuda is available\n",
    "    if device == \"cuda\":\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"CUDA is not available, using CPU instead\")\n",
    "            device = \"cpu\"\n",
    "\n",
    "    print(f\"Running on {device}\")\n",
    "    print(f\"Size: {size}\")\n",
    "    print(f\"Entries: {entries}\")\n",
    "    print(f\"Synchronize GPU: {syncgpu}\")\n",
    "    \n",
    "    # Generate the matrices\n",
    "    matrices = generate(size, entries, device=device)\n",
    "\n",
    "    # Size of the RHS\n",
    "    x_cols = 100\n",
    "\n",
    "    # Compute the timings\n",
    "    print('Matrix-matrix multiplication timings:')\n",
    "    base_time = None\n",
    "    base_name = None\n",
    "    all_times = {}\n",
    "    for matrix, matrix_name in zip(matrices.values(), matrices.keys()):\n",
    "        # first, do a few runs to warm up the cache\n",
    "        for i in range(2):\n",
    "            x = torch.randn(size, x_cols, device=device)\n",
    "            y = matrix @ x\n",
    "        # now do the timings\n",
    "        matrix_times = []\n",
    "        for i in range(5):\n",
    "            x = torch.randn(size, x_cols, device=device)    \n",
    "            if syncgpu:\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            y = matrix @ x\n",
    "            if syncgpu:\n",
    "                torch.cuda.synchronize()\n",
    "            matrix_time = time.perf_counter()-start\n",
    "            matrix_times.append(matrix_time)\n",
    "        avg_matrix_time = sum(matrix_times)/len(matrix_times)\n",
    "        min_matrix_time = min(matrix_times)\n",
    "        max_matrix_time = max(matrix_times)\n",
    "        if base_time is None:\n",
    "            base_time = avg_matrix_time\n",
    "            base_name = matrix_name\n",
    "\n",
    "        all_times[matrix_name] = avg_matrix_time\n",
    "\n",
    "        print(f\"{matrix_name} avg time:\", avg_matrix_time)\n",
    "        print(f\"{matrix_name} min time:\", min_matrix_time)\n",
    "        print(f\"{matrix_name} max time:\", max_matrix_time)\n",
    "        print(f\"{matrix_name} speedup over {base_name}:\", base_time/avg_matrix_time)\n",
    "\n",
    "    return all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Size: 20000\n",
      "Entries: 2000000\n",
      "Synchronize GPU: True\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 23.64 GiB of which 556.62 MiB is free. Process 2099963 has 20.99 GiB memory in use. Including non-PyTorch memory, this process has 2.05 GiB memory in use. Of the allocated memory 1.57 GiB is allocated by PyTorch, and 33.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_times \u001b[38;5;241m=\u001b[39m \u001b[43mrun_timing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mrun_timing\u001b[0;34m(size, entries, device, syncgpu)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynchronize GPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msyncgpu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate the matrices\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m matrices \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Size of the RHS\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(size, entries, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(x\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 32\u001b[0m maskedLinear \u001b[38;5;241m=\u001b[39m moduleWrapper(\u001b[43mMaskedLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_coo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), device)\n\u001b[1;32m     33\u001b[0m sparseLinear \u001b[38;5;241m=\u001b[39m moduleWrapper(SparseLinear\u001b[38;5;241m.\u001b[39mfrom_coo(coo)\u001b[38;5;241m.\u001b[39mto(device), device)\n\u001b[1;32m     35\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dense\n",
      "File \u001b[0;32m~/projects/2_research/iterativennsimple/iterativennsimple/MaskedLinear.py:198\u001b[0m, in \u001b[0;36mMaskedLinear.from_coo\u001b[0;34m(coo, check_mask, bias, device, dtype)\u001b[0m\n\u001b[1;32m    193\u001b[0m A \u001b[38;5;241m=\u001b[39m MaskedLinear(in_features\u001b[38;5;241m=\u001b[39mcoo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m    194\u001b[0m                  out_features\u001b[38;5;241m=\u001b[39mcoo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    195\u001b[0m                  bias\u001b[38;5;241m=\u001b[39mbias, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# The weights are just the COO matrix in dense form\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     A\u001b[38;5;241m.\u001b[39mweight_0[:, :] \u001b[38;5;241m=\u001b[39m \u001b[43mcoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# The mask is 1 where the COO matrix is non-zero\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     A\u001b[38;5;241m.\u001b[39mmask[:, :] \u001b[38;5;241m=\u001b[39m (A\u001b[38;5;241m.\u001b[39mweight_0[:, :] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 23.64 GiB of which 556.62 MiB is free. Process 2099963 has 20.99 GiB memory in use. Including non-PyTorch memory, this process has 2.05 GiB memory in use. Of the allocated memory 1.57 GiB is allocated by PyTorch, and 33.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "all_times = run_timing(20000, 100*20000, \"cuda\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a dataframe with row and column names for the heatmap\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m\u001b[43mall_times\u001b[49m\u001b[38;5;241m.\u001b[39mkeys(), index\u001b[38;5;241m=\u001b[39mall_times\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fill in df with the values we want to display\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name1, time1 \u001b[38;5;129;01min\u001b[39;00m all_times\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_times' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with row and column names for the heatmap\n",
    "df = pd.DataFrame(columns=all_times.keys(), index=all_times.keys())\n",
    "\n",
    "# Fill in df with the values we want to display\n",
    "for name1, time1 in all_times.items():\n",
    "    for name2, time2 in all_times.items():\n",
    "        df.loc[name1, name2] = time1/time2\n",
    "fig = px.imshow(df, text_auto=True, color_continuous_scale='RdBu')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
